{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7ec5cd",
   "metadata": {},
   "source": [
    "\n",
    "# INTRODUCTION TO CLASSIFICATION\n",
    "\n",
    "Work plan:\n",
    "\n",
    "* Read in the data\n",
    "* Split the data into **training** and **test** sets\n",
    "* Build a model using the **training** set\n",
    "* Evaluate the model using the **test** set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42294c",
   "metadata": {},
   "source": [
    "Please download the data file \"players.txt\" into a local directory.\n",
    "\n",
    "We are going to use the sklearn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ca73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72164233",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv(\"players.txt\")\n",
    "players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4df523",
   "metadata": {},
   "outputs": [],
   "source": [
    "players.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb5cad",
   "metadata": {},
   "source": [
    "# ATTENTION\n",
    "\n",
    "The \"id\" attribute uniquely identifies a player within our data set. \n",
    "This attribute cannot be used to classify new players as each player has a different id number. It is always important to check whether the data contains unnecessary attributes.\n",
    "\n",
    "Before we continue, we'll remove the \"id\" attribute from our data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.drop('id', axis=1)\n",
    "players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca05770",
   "metadata": {},
   "source": [
    "# Example 1 - Prediction of playing position\n",
    "\n",
    "\n",
    "We want to build a model to predict a player's playing position \n",
    "with respect to the given player's statistics. The target variable \"position\" is discrete - we term this a classification task. We aim to verify whether or not it is possible to use historical data to predict\n",
    "playing positions for new players. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af833c",
   "metadata": {},
   "source": [
    "\n",
    "We are going to split the data into a training and testing data set.\n",
    "The training data set consists of players that ended their careers before 1999.\n",
    "The test data set consists of players that began their careers after 1999.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e90b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "players['lastseason'] <= 1999\n",
    "train = players[players['lastseason'] <= 1999]\n",
    "test = players[players['firstseason'] > 1999]\n",
    "print(\"Number of training examples:\", len(train))\n",
    "print(\"Number of test examples:\", len(test))\n",
    "print(\"ratio of train examples:\", len(train)/(len(train)+len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e0105",
   "metadata": {},
   "source": [
    "We used the \"firstseason\" and \"lastseason\" attributes to split the data. Therefore the attributes are not going to contribute to the modelling task, so we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316982cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('lastseason', axis=1)\n",
    "train = train.drop('firstseason', axis=1)\n",
    "\n",
    "test = test.drop('lastseason', axis=1)\n",
    "test  = test.drop('firstseason', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5125f",
   "metadata": {},
   "source": [
    "Lets inspect the player positions in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df88099",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbad107",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['position'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0dd08a",
   "metadata": {},
   "source": [
    "# Majority classifier\n",
    "\n",
    "The majority class is the class with the highest number of training examples. This is the simplest classifier can be used as a baseline for comparison with other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_index = train['position'].value_counts().argmax()\n",
    "\n",
    "majority_class = train['position'].value_counts().index[majority_index]\n",
    "\n",
    "print(majority_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97ba90",
   "metadata": {},
   "source": [
    "How well does this perform on the test set?\n",
    "\n",
    "We can evaluate model performance using classification accuracy (the percentage of correct predictions on the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = test[test['position'] == majority_class]\n",
    "CA = len(correct_predictions) / len(test)\n",
    "CA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11295b46",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "The goal of decision trees is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. Each node of a decision tree splits the dataset according to a decision rule.\n",
    "\n",
    "## Some advantages of decision trees are:\n",
    "\n",
    "* Simple to understand and to interpret. Trees can be visualized.\n",
    "* Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Some tree and algorithm combinations support missing values.\n",
    "* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "* Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now.\n",
    "* Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "\n",
    "## Some disadvantages of decision trees include:\n",
    "\n",
    "* Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5748332",
   "metadata": {},
   "source": [
    "All sklearn classifiers can be used with the following steps:\n",
    "\n",
    "* Create a classifier object\n",
    "* Fit the classifier with .fit(X, Y) on the TRAIN set\n",
    "* Obtain the predictions with .predict(X) on the TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a83e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('position', axis=1)    # Everything except the class\n",
    "Y_train = train['position']                 # Just the class\n",
    "\n",
    "X_test = test.drop('position', axis=1)\n",
    "Y_test = test['position']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c31d8",
   "metadata": {},
   "source": [
    "Error - our dataset contains missing values, which can cause errors when training classifiers. We remove missing values before training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c77113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth controls the maximum depth of the decision tree. Smaller trees are less complex but can help prevent overfitting\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 3)   \n",
    "clf.fit(train.drop('position', axis=1), train['position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e48fab",
   "metadata": {},
   "source": [
    "Visualize the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=list(train.drop('position', axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.export_text(clf, feature_names = column_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340d009",
   "metadata": {},
   "source": [
    "After training, obtain predictions with .predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32248f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d131f",
   "metadata": {},
   "source": [
    "How good is the model?\n",
    "\n",
    "Let's calculate the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = (preds == Y_test)\n",
    "correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e663ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "CA = sum(correct_predictions) / len(preds)\n",
    "CA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69853d",
   "metadata": {},
   "source": [
    "We can examine the results further using a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea48eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true vaule / predicted value\n",
    "#    G   F   C\n",
    "# G  x   x   x\n",
    "# F  x   x   x\n",
    "# C  x   x   x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d013c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = sklearn.metrics.confusion_matrix(preds, Y_test)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b45931",
   "metadata": {},
   "source": [
    "We can also obtain the CA by summing the diagonal of a confusion matrix (the correct predictions) and dividing it with the sum of the whole matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.diag(conf_mat)) / np.sum(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e41c5",
   "metadata": {},
   "source": [
    "Addionally, we can obtain class probabilities instead of discrete predictions.\n",
    "\n",
    "This can be used to calculate alternative scores, such as the Brier score, which also takes into account class probabilities and returns a value between 0 (best) and 1 (worst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_proba = clf.predict_proba(X_test)\n",
    "preds_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6da601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode ground truth classes to bits\n",
    "# Sklearn encoders can be used to change data in various ways\n",
    "encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "# Encoders require a 2D array so reshape is needed\n",
    "Y_test_proba = encoder.fit_transform(np.array(Y_test).reshape(-1, 1))\n",
    "Y_test_proba.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b53017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(preds, ground_truth):\n",
    "    return np.sum((ground_truth - preds) ** 2) / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f3c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brier_score(preds_proba, np.array(Y_test_proba.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22ba1e",
   "metadata": {},
   "source": [
    "# Example 2\n",
    "\n",
    "Does a player make more than 80% of free-throws attempted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea23f91",
   "metadata": {},
   "source": [
    "This is a binary problem (the target variable is discrete with values YES and NO). We do not have this attribute, so we will need to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only players who have made at least 1 free throw\n",
    "bin_players = players[players['fta'] > 0].copy()\n",
    "\n",
    "# Calculate free-throw success rate\n",
    "free_throw_rate = bin_players['ftm'] / bin_players['fta']\n",
    "\n",
    "# Create a discrete attribute \"ftexpert\". This will be our target variable.\n",
    "ftexpert = pd.cut(free_throw_rate, [-1, 0.8, 1], labels=[\"NO\", \"YES\"])\n",
    "\n",
    "# Assign the new attribute to a new column\n",
    "bin_players['ftexpert'] = ftexpert\n",
    "\n",
    "bin_players = bin_players.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff54bab",
   "metadata": {},
   "source": [
    "**Important:** The new ftexpert attribute is based on two existing attributes (fta, ftm). Therefore, it would be very easy to predict if we left these two attributes in the dataset. These two attributes must be removed.\n",
    "\n",
    "**Important:** DecisionTreeClassifier needs all attributes to be numbers. Here, the \"position\" attribute is a string (either \"G\", \"C\", or \"F\") so it must first be converted to a number. This can be done using pd.get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bfd279",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_players = bin_players.drop('fta', axis=1)\n",
    "bin_players = bin_players.drop('ftm', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84346554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_players['position'] = bin_players['position'].astype('category')\n",
    "bin_players = pd.get_dummies(bin_players, columns = [\"position\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44fba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_players.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_players[['position_C','position_G','position_F']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16887fe8",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets. This time using a built-in sklearn method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5596bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bin_players.drop('ftexpert', axis=1)\n",
    "Y = bin_players['ftexpert']\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, train_size = 0.8)\n",
    "print(len(X_train), len(X_test), len(X_train)/(len(X_train) + len(X_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4ca911",
   "metadata": {},
   "source": [
    "Inspect the distribution of the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c093076",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train.value_counts())\n",
    "print(Y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49485647",
   "metadata": {},
   "source": [
    "Train a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_clf = tree.DecisionTreeClassifier(max_depth=5)\n",
    "bin_clf.fit(X_train, Y_train)\n",
    "preds = bin_clf.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4465044",
   "metadata": {},
   "source": [
    "Calculate accuracy using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d950c8",
   "metadata": {},
   "source": [
    "Compare to majority (using sklearn DummyClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e309b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f66787",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_classifier =  DummyClassifier()\n",
    "majority_classifier.fit(X_train, Y_train)\n",
    "majority_preds = majority_classifier.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(majority_preds, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326911b5",
   "metadata": {},
   "source": [
    "# Sensitivity, Specificity and ROC curve\n",
    "\n",
    "These three metrics can be used to further examine classification results.\n",
    "\n",
    "* Sensitivity: Correct positive predictions (TP) out of all positive examples (P): TP/P\n",
    "* Specificity: Correct negative predictions (TN) out of all negative examples (N): TN/N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sensitivity\", sum((preds == 'YES') & (Y_test == \"YES\")) / sum((Y_test == \"YES\")))\n",
    "print(\"Specificity\", sum((preds == 'NO') & (Y_test == \"NO\")) / sum((Y_test == \"NO\"))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b62491",
   "metadata": {},
   "source": [
    "We can also obtain a set of scores using sklearn.metrics.classification_report. Here, sensitivity and specificity are present under different names. The relevant row is \"YES\", which shows the results if YES is treated as the positive class. In this function, sensitivity is reffered to as recall while specificity is the recall if we treat \"NO\" as the positive class (in the \"NO\" row).\n",
    "\n",
    "We also see some additional scores:\n",
    "\n",
    "* Precision, which is the number of correct positive predictions out of all positive predictions: TP/(TP + FP)\n",
    "* F1-score, which is the harmonic mean of precision and recall ((2 * precision * recall)/(precision + recall))\n",
    "* Support, which is the number of occurances of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b893685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(Y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd923c",
   "metadata": {},
   "source": [
    "Since Sensitivity and specificity are based on the number of positive and negative predictions, we can vary both by changing the prediction threshold.\n",
    "\n",
    "First, predict probabilities instead of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7988981",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_proba = bin_clf.predict_proba(X_test)\n",
    "preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396d913",
   "metadata": {},
   "source": [
    "With the basic .predict function, the threshold for a prediction is 0.5. If the probability of a class is >= 0.5, that class is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_threshold = 0.5\n",
    "# x[0] is the first value in each row of preds_proba, corresponding to the no class\n",
    "threshold_predictions = ['NO' if x[0] >= no_threshold else 'YES' for x in preds_proba]   \n",
    "all(threshold_predictions == preds)    # Our new predictions are the same as predictions obtained with .predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3d45d",
   "metadata": {},
   "source": [
    "If we want more positive predictions, we can increase the threshold for the \"NO\" class. This will lead to less negative and more positive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9902c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_threshold = 0.8   # Now only examples where the probability for the no class is >= 0.8 will be labelled as \"NO\"\n",
    "threshold_predictions = np.array(['NO' if x[0] >= no_threshold else 'YES' for x in preds_proba])\n",
    "print(\"Threshold predictions\", Counter(threshold_predictions))\n",
    "print(\"Predictions from .predict()\", Counter(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e0ced",
   "metadata": {},
   "source": [
    "More positive predictions means we (likely) increase the number of true positives (TP) while the number of all positives (P) remains the same. Therefore, the sensitivity (TP/P) increases. The reverse is true for sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda589f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sensitivity\", sum((threshold_predictions == 'YES') & (Y_test == \"YES\")) / sum((Y_test == \"YES\")))\n",
    "print(\"Specificity\", sum((threshold_predictions == 'NO') & (Y_test == \"NO\")) / sum((Y_test == \"NO\"))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18fea0",
   "metadata": {},
   "source": [
    "We can visualize all possible ratious between sensitivity and specificity using a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ee49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve in text form (less common)\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(Y_test, [x[1] for x in preds_proba], pos_label=\"YES\")\n",
    "print(fpr)\n",
    "print(tpr)\n",
    "print(thresholds)\n",
    "\n",
    "# ROC curve in graph form\n",
    "RocCurveDisplay.from_estimator(bin_clf, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdf87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or from results\n",
    "# The function takes only the probabilites of the positive class, hence [x[1] for x in preds_proba]\n",
    "RocCurveDisplay.from_predictions(Y_test, [x[1] for x in preds_proba], pos_label=\"YES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee524c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the AUC score\n",
    "sklearn.metrics.roc_auc_score(Y_test, [x[1] for x in preds_proba])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad25fc",
   "metadata": {},
   "source": [
    "A bigger area under the curve (AUC - area under curve), the better the results. The top left point would indicate a scenario where both sensitivity and specificity equal 1 - the perfect result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
