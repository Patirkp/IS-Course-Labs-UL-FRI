{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd2263c",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Unlike supervised learning (classification/regression), reinforcement learning does not require labelled data. Instead, training is performed through **exploration** and **exploitation** of a given environment. \n",
    "\n",
    "The environment commonly consists of different states, where each state can be assigned a reward depending on how useful the state is for achieving the desired solution. The goal of RL algorithms is to find solutions that achieve the maximal reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad7f65",
   "metadata": {},
   "source": [
    "# Example 1 - Manual implementation\n",
    "\n",
    "A straightforward way to implement reinforcement learning is to define the environment with a number of states, where each state is assigned a number. In our example, we will apply reinforcement learning to a maze-solving algorithm where each space is assigned a sequential number:\n",
    "\n",
    "    S..   0 1 2\n",
    "    ...   3 4 5\n",
    "    E..   6 7 8\n",
    "\n",
    "Where S represents the start and E represends the exit. Additionally, we will define 4 possible moves, where each will also have a corresponding number:\n",
    "\n",
    "    possible moves\n",
    "\n",
    "        0\n",
    "      3   1\n",
    "        2\n",
    "\n",
    "With this, we can define our problem space using a matrix of all possible transitions between spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [['S', '.', '.'],\n",
    "        ['.', '.', '.'],\n",
    "        ['E', '.', '.']]\n",
    "\n",
    "rewards = np.ones((len(maze)*len(maze[0]), 4))*-1\n",
    "transitions = np.ones((len(maze)*len(maze[0]), 4))*-1\n",
    "\n",
    "\n",
    "transitions = transitions.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "transitions[0][1] = 1\n",
    "transitions[0][2] = 3\n",
    "transitions[1][1] = 2\n",
    "transitions[1][2] = 4\n",
    "transitions[1][3] = 0\n",
    "transitions[2][2] = 5\n",
    "transitions[2][3] = 1\n",
    "transitions[3][0] = 0\n",
    "transitions[3][1] = 4\n",
    "transitions[3][2] = 6\n",
    "transitions[4][0] = 1\n",
    "transitions[4][1] = 5\n",
    "transitions[4][2] = 7\n",
    "transitions[4][3] = 3\n",
    "transitions[4][0] = 2\n",
    "transitions[5][2] = 8\n",
    "transitions[5][3] = 4\n",
    "transitions[6][0] = 3\n",
    "transitions[6][1] = 7\n",
    "transitions[7][0] = 4\n",
    "transitions[7][1] = 8\n",
    "transitions[7][3] = 6\n",
    "transitions[8][0] = 5\n",
    "transitions[8][3] = 7\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f5f36",
   "metadata": {},
   "source": [
    "Additionally, we can define the rewards for each move. To keep this simple, we will reward the moves that reach the exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[3][1] = 10\n",
    "rewards[7][3] = 10\n",
    "\n",
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd3b3e",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "The value iteration method attempts to approximate the optimal value function for each possible state. It scores each state so that states that are more likely to lead to larger rewards get a larger value. To compute this, we need to know the possible state transitions ahead of time. In our case, this is the matrix transitions. Then, we iteratively update the score of each state based on the best possible move we can make from this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration method\n",
    "def value_iteration(T, R, gamma = 0.9, iterations = 100):\n",
    "    #initialize starting state values to 0\n",
    "    # V - The vector of scores for each state\n",
    "    V = np.zeros(len(R))\n",
    "    # a set of actions to iterate through\n",
    "    A = list(range(len(R[0])))\n",
    "    # a set of states to iterate through\n",
    "    S = list(range(len(R)))\n",
    "  \n",
    "    # Q - The vector of scores for each [state, action] pair\n",
    "    # initialize the matrix Q (states X actions)\n",
    "    # everything should start at 0\n",
    "    Q = np.zeros((len(R), len(R[0])))\n",
    "  \n",
    "    # Iterate trough every state and action multiple times\n",
    "    for i in range(iterations):\n",
    "        #for each state \"s\"\n",
    "        for s in S:\n",
    "            #for each action \"a\"\n",
    "            for a in A:\n",
    "                # Update the score for the action \"a\" from the state \"s\".\n",
    "                # It should be equal to the score in the reward matrix R + \n",
    "                # some factor gamma times the score of the state we reached\n",
    "                # with that action. Gamma is the discount factor and should \n",
    "                # be set to a number lower than 1. In a maze-solving problem,\n",
    "                # this ensures that the further away a room is from the exit,\n",
    "                # the smaller its score will be.\n",
    "                #print(T)\n",
    "                if T[s, a] == -1:\n",
    "                    print(s, a)\n",
    "                    continue\n",
    "                Q[s,a] = R[s,a] + gamma*1*V[T[s,a]]\n",
    "                # Update the scores for the state. The score is equal to the\n",
    "                # score of the best possible move we can make from that state.\n",
    "                V[s] = max(Q[s])\n",
    "    # Return the scores of each state\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_iteration(transitions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0593fad",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Q-learning is similar to value iteration but does not update all the states in each iteration. Instead, it uses an agent that randomly explores the environment. When it moves from state A to state B it increases or decreases the score of state A based on the reward that move obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning\n",
    "def qlearning(T, R, F, gamma = 0.9):\n",
    "\n",
    "    # the number of states\n",
    "    nstates = len(T)\n",
    "    nactions = len(T[0])\n",
    "  \n",
    "    # initialize the matrix Q\n",
    "    Q = np.zeros((nstates, nactions))\n",
    "  \n",
    "    alpha = 1\n",
    "  \n",
    "    while alpha > 0.1:\n",
    "  \n",
    "        # We want an agent that randomly explores the environment\n",
    "        # randomly select a starting state\n",
    "        cur_state = np.random.choice(list(range(nstates)))\n",
    "    \n",
    "        # move around until we reach one of the final states\n",
    "        while cur_state not in F:\n",
    "    \n",
    "            # see which actions are possible in the current state\n",
    "            possible_actions = [x[0] for x in enumerate(T[cur_state]) if x[1] != -1]\n",
    "      \n",
    "            # randomly select the next action\n",
    "            action = np.random.choice(possible_actions)\n",
    "      \n",
    "            # the selected action determines the next state\n",
    "            next_state = T[cur_state, action]\n",
    "      \n",
    "            # update Q for the current state and the selected action\n",
    "            # This is similar to value iteration, with some changes:\n",
    "            #     - We no longer use the V vector. Instead this is incorporated\n",
    "            #       directly into the equation.\n",
    "            #     - We use alpha to ensure later iterations don't chage the Q\n",
    "            #       matrix as much as earlier iterations\n",
    "            #     - We subtract Q[cur.state, action] at the end to ensure moving\n",
    "            #       from a better state to a worse state is penalized even if \n",
    "            #       the worse state still has a good score.\n",
    "            Q[cur_state][action] = Q[cur_state][action] + alpha * (R[cur_state][action] + gamma * np.max(Q[next_state]) - Q[cur_state, action])\n",
    "      \n",
    "            # Execute the move\n",
    "            cur_state = next_state\n",
    "    \n",
    "    \n",
    "        # Lower alpha during every iteration\n",
    "        alpha = alpha * 0.999\n",
    "  \n",
    "  \n",
    "    # Return the Q matrix, which is normalized so that the scores look nicer.\n",
    "    return Q / np.max(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74929a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearning(transitions, rewards, [6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914505e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3466e2",
   "metadata": {},
   "source": [
    "# Example 2 - OpenAI gymnasium library\n",
    "\n",
    "In the previous example, we defined the problem space as a matrix of all possible state transitions. However, this can be impractical for larger problems. Instead, we can represent as python classes. In this example, we will use the gymnasium library:\n",
    "\n",
    "    Gymnasium is an open source Python library for developing and comparing reinforcement learning algorithms by   \n",
    "    providing a standard API to communicate between learning algorithms and environments, as well as a standard set of \n",
    "    environments compliant with that API. \n",
    "    \n",
    "The library represents environments using the gym.Env class. Each environment must contain the following main methods:\n",
    "\n",
    "\n",
    "    step() - Updates an environment with actions returning the next agent observation, the reward for taking that actions, if the environment has terminated or truncated due to the latest action and information from the environment about the step, i.e. metrics, debug info.\n",
    "\n",
    "    reset() - Resets the environment to an initial state, required before calling step. Returns the first agent observation for an episode and information, i.e. metrics, debug info.\n",
    "\n",
    "    render() - Renders the environments to help visualise what the agent see, examples modes are “human”, “rgb_array”, “ansi” for text.\n",
    "\n",
    "    close() - Closes the environment, important when external software is used, i.e. pygame for rendering, databases\n",
    "\n",
    "Let's see how gymnasium implements a simple maze problem (without walls):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.vector.utils import batch_space\n",
    "import pygame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        # We will use action_space to sample moves\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Gymnasium allows us to display our problem in a human-friendly way.\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate the starting and ending locations. We use a generator with a set seed so that\n",
    "        we always get the same maze.\n",
    "        \"\"\"\n",
    "        self.generator, _ = gym.utils.seeding.np_random(seed=3025)\n",
    "        \n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._starting_agent_location = self.generator.integers(0, self.size, size=2, dtype=int)\n",
    "        self._agent_location = self._starting_agent_location\n",
    "        \n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.generator.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "            \n",
    "    # Observations encode the current state of our problem. In this case, we will\n",
    "    # represent this with the locations of the agent and target\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    \n",
    "    # Additionally, we can return useful information. We won't be using this,\n",
    "    # but we could use the distance to target location as a possible reward/penalty\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "    reset() initializes the starting state of our game. Here, we move the agent to\n",
    "    the starting position\n",
    "    \"\"\"\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self._agent_location = self._starting_agent_location\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        \n",
    "        # reset() always returns the observation (the state of our game) and \n",
    "        # auxillary information\n",
    "        return observation, info\n",
    "    \n",
    "    \"\"\"\n",
    "    step() tells us what happens during each step of our game. In our case, we need to\n",
    "    pick a random direction and move the agent in that direction.\n",
    "    \n",
    "    Additionally, step() also calculates the rewards of our action. In this case, we will\n",
    "    simply reward each step that moves into the target.\n",
    "    \"\"\"\n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        \n",
    "        # An episode is done if the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        \"\"\"\n",
    "        step() returns 5 values:\n",
    "            * observation: the state of the environment after the step\n",
    "            * reward: the reward of the step\n",
    "            * terminated: whether the step ended the game\n",
    "            * truncated: whether the game ended in an unusual way. In our case, this is always False\n",
    "            * info: additional auxiliary information. In our case, the distance to the target\n",
    "        \"\"\"\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    \"\"\"\n",
    "    render() is used to display the game state in a human friendly way. In our case, we use\n",
    "    the pygame library to draw the current state.\n",
    "    \"\"\"\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        \n",
    "    \"\"\"\n",
    "    close() cleans up the environment. In our case we simply close the pygame window\n",
    "    \"\"\"\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64547e78",
   "metadata": {},
   "source": [
    "To run our environment we first create the class and call reset() to generate a new game.\n",
    "\n",
    "We then generate each move using env.action_space.sample() and apply the move using env.step(action). If the game has ended, we call reset() again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv(render_mode=\"human\")\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9dede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17977c49",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "\n",
    "At this point, every action in our game is entirely random (we generate them using env.action_space.sample()). In order to train our agent, we again need to implement some version of reinforcement learning. We will do this by creating a new class (MazeAgent) that will interact with the  GridWorldEnv environment. This class will implement q-learning using two key methods:\n",
    "\n",
    "    get_actions(obs): Instead of randomly sampling actions, get_actions will return the optimal action based on the learned policy\n",
    "    \n",
    "    update(obs, action, reward, terminated, next_obs): This function will update the Q-value of a given action (i.e., transition from obs -> next_obs) based on the reward obtained by the action\n",
    "    \n",
    "We will also no longer save the Q-values into a matrix. Instead, we will use the defaultdict class which acts as a normal dictionary when given a valid key or returns a 0 if the key is not in a dictionary. We will use this to save the computed q-values while returning 0 for actions without an existing q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbbba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MazeAgent:\n",
    "        def __init__(self, \n",
    "                     learning_rate=0.01,\n",
    "                     initial_epsilon=1.0,\n",
    "                     epsilon_decay=1.0 / (10000 / 2),\n",
    "                     final_epsilon=0.1,\n",
    "                     discount_factor=0.95):\n",
    "\n",
    "            \"\"\"\n",
    "            Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "            of state-action values (q_values), a learning rate and an epsilon.\n",
    "            \n",
    "            \n",
    "\n",
    "            Args:\n",
    "                learning_rate: The learning rate\n",
    "                initial_epsilon: The initial epsilon value\n",
    "                epsilon_decay: The decay for epsilon\n",
    "                final_epsilon: The final epsilon value\n",
    "                discount_factor: The discount factor for computing the Q-value\n",
    "            \"\"\"\n",
    "            self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "            self.lr = learning_rate\n",
    "            self.discount_factor = discount_factor\n",
    "\n",
    "            self.epsilon = initial_epsilon\n",
    "            self.epsilon_decay = epsilon_decay\n",
    "            self.final_epsilon = final_epsilon\n",
    "\n",
    "            self.training_error = []\n",
    "    \n",
    "        def get_action(self, obs):\n",
    "            \"\"\"\n",
    "            Returns the best action with probability (1 - epsilon)\n",
    "            otherwise a random action with probability epsilon to ensure exploration.\n",
    "            \"\"\"\n",
    "            # with probability epsilon return a random action to explore the environment\n",
    "            if np.random.random() < self.epsilon:\n",
    "                return env.action_space.sample()\n",
    "\n",
    "            # with probability (1 - epsilon) act greedily (exploit)\n",
    "            # Look at the q_values for a given observation and return the best action\n",
    "            else:\n",
    "                return int(np.argmax(self.q_values[str(obs)]))\n",
    "\n",
    "        def update( self,\n",
    "                    obs,\n",
    "                    action,\n",
    "                    reward,\n",
    "                    terminated,\n",
    "                    next_obs):\n",
    "            \"\"\"Updates the Q-value of an action usint the q-learning equation.\"\"\"\n",
    "            \n",
    "            #future_q_value = (not terminated) * np.max(self.q_values[str(next_obs)])\n",
    "            future_q_value = np.max(self.q_values[str(next_obs)])\n",
    "            #print(not terminated, future_q_value, np.max(self.q_values[str(next_obs)]), self.epsilon)\n",
    "            temporal_difference = (\n",
    "                reward + self.discount_factor * future_q_value - self.q_values[str(obs)][action]\n",
    "            )\n",
    "\n",
    "            self.q_values[str(obs)][action] = (\n",
    "                self.q_values[str(obs)][action] + self.lr * temporal_difference\n",
    "            )\n",
    "            self.training_error.append(temporal_difference)\n",
    "            \n",
    "            \n",
    "        def decay_epsilon(self):\n",
    "            self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e18487",
   "metadata": {},
   "source": [
    "Now we can train our agent. First, initialize the MazeAgent object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89823231",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_episodes = 1000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "\n",
    "agent = MazeAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78099449",
   "metadata": {},
   "source": [
    "Then train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm adds a nice progress bar to any loop\n",
    "# unnecessary, but nice to have\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = GridWorldEnv()\n",
    "#env.render_mode=\"human\"\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        #print(obs)\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()\n",
    "#print(agent.q_values[\"{'agent': array([1, 2]), 'target': array([4, 3])}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd150b3c",
   "metadata": {},
   "source": [
    "Let's check if the training worked. We will generate the actions using the trained agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f69673",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render_mode=\"human\"\n",
    "\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = agent.get_action(obs) \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f371a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50158c3",
   "metadata": {},
   "source": [
    "## Extending the enviromnent\n",
    "\n",
    "Using the gymnasium library, we can easily extend environments with additional features. For example, let's add walls to our environment. We will need to:\n",
    "\n",
    "* Update the init() function to generate walls\n",
    "* Update the step() function to penalize the agent running into walls\n",
    "* Update the _render_frame() function to draw walls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d65324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldWallsEnv(GridWorldEnv):\n",
    "    def __init__(self, render_mode=None, size=5, num_walls = 1):\n",
    "        super().__init__(render_mode, size)\n",
    "        self.num_walls = num_walls\n",
    "        \n",
    "        # We now also generate walls by randomly sampling until we get empty locations\n",
    "        self._wall_locations = []\n",
    "        for i in range(self.num_walls):\n",
    "            wall_location = self.generator.integers(0, self.size, size=2, dtype=int)\n",
    "            while (np.array_equal(wall_location, self._agent_location) \n",
    "                   or np.array_equal(wall_location, self._target_location)\n",
    "                   or any([np.array_equal(wall_location, x) for x in self._wall_locations])):\n",
    "                    wall_location = self.generator.integers(0, self.size, size=2, dtype=int)\n",
    "                    \n",
    "            self._wall_locations.append(wall_location)\n",
    "        \n",
    "    def step(self, action):\n",
    "       \n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        \n",
    "        # Additionally, let's end the episode if the agent hits a wall\n",
    "        terminated_wall = any([np.array_equal(self._agent_location, x) for x in self._wall_locations])\n",
    "        if terminated:\n",
    "            reward = 1\n",
    "        elif terminated_wall:\n",
    "            reward = -0.1\n",
    "        else:\n",
    "            reward = 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated or terminated_wall, False, info\n",
    "    \n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "        \n",
    "        # Now we draw the walls\n",
    "        for wall_loc in self._wall_locations:\n",
    "            pygame.draw.rect(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * wall_loc,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,)\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldWallsEnv(render_mode=\"human\", num_walls = 12)\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cc7f7",
   "metadata": {},
   "source": [
    "Since we're still using the gymnasium environment class, we can reuse the previous RL agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799428c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_episodes = 10000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "\n",
    "agent = MazeAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "\n",
    "env = GridWorldWallsEnv(num_walls = 12)\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    agent.decay_epsilon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a515de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.render_mode=\"human\"\n",
    "obs, info = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = agent.get_action(obs) \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d32c98",
   "metadata": {},
   "source": [
    "## Additional environments\n",
    "\n",
    "We can also apply our Q-learning class to other environments. Since it only requires the actions and rewards provided by the environment class, we can apply it to any gymnasium environment without modifications.\n",
    "\n",
    "For example, let's see if it works on the built-in *LunarLander* environment: a physics-based game where the objective is to land a spacecraft on the moon. This environment is more complex than the maze but works on the same overall principle: for each step we give it an action which returns the next state + reward. The class is very well described in the documentation: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/lunar_lander.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eff665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5480a5",
   "metadata": {},
   "source": [
    "Let's see how it works without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee10fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf4c23",
   "metadata": {},
   "source": [
    "It seems to crash pretty quickly. Let's see if q-learning can improve this. This is the exact same code we used for the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "660e5be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 20000/20000 [17:23<00:00, 19.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make(\"LunarLander-v2\", render_mode=None)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_episodes = 20000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "\"\"\"\n",
    "agent = MazeAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "env.unwrapped.render_mode = None\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        #print(obs)\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "env.unwrapped.render_mode = \"human\"\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    image = env.render()\n",
    "    #print(image)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "878976d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
